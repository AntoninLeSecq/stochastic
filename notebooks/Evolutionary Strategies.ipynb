{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Evolutionary Strategies\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-sa/4.0/80x15.png\" /></a>&nbsp;| Dennis G. Wilson | <a href=\"https://supaerodatascience.github.io/stochastic/\">https://supaerodatascience.github.io/stochastic/</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview\n",
    "\n",
    "1. [$(1 + \\lambda)$ ES](#oneplus)\n",
    "2. [Approximating the gradient](#gradient)\n",
    "3. [Canonical ES](#canonical)\n",
    "4. [CMA-ES part 1: maximum likelihood estimation](#cmaes)\n",
    "5. [CMA-ES part 2: step size adaptation](#step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <a name=\"oneplus\">1.</a> The $(1 + \\lambda)$ ES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The simplest Evolutionary Strategy is the $(1+1)$. In this algorithm, the best individual generated is replaced by a new individual when its **fitness** is superior to the current best.\n",
    "\n",
    "    Initialize x randomly in ‚Ñù\n",
    "    while not terminate\n",
    "        x' = x + ùëÅ(0, 1)\n",
    "        if f(x‚Ä≤) < f(x)\n",
    "            x = x'\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this algorithm, at each step, we randomly sample from a Normal distribution to get one new point near the current point. However, to better inform the movement from this initial point, we could sample multiple points instead of a single one. We'll first look at examples of continuous functions, as their search space is easier to visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def himmelblau(x, y):\n",
    "    return (x**2 + y - 11)**2 + (x + y**2 - 7)**2\n",
    "\n",
    "def himmelblau_v(x):\n",
    "    return himmelblau(x[0], x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def rosenbrock(x, y, a=1, B=100):\n",
    "      return (a-x)**2 + B*((y-x**2))**2\n",
    "    \n",
    "def rosenbrock_v(x):\n",
    "      return rosenbrock(x[0], x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def warped_rastrigin(x, y):\n",
    "    x = (x + 2.7) / 3\n",
    "    y = (y - 4.1) / 2\n",
    "    return 20 + x**2 - 10 * np.cos(2*np.pi*x) + y**2 - 10 * np.cos(2*np.pi*y)\n",
    "\n",
    "def warped_rastrigin_v(x):\n",
    "    return warped_rastrigin(x[0], x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker, cm\n",
    "import matplotlib.colors as colors\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = np.arange(-5, 5, 0.1)\n",
    "Y = np.arange(-5, 5, 0.1)\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "Z = himmelblau(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6))\n",
    "cs = plt.contour(X, Y, Z, levels=100, cmap='Spectral',\n",
    "                 norm=colors.Normalize(vmin=Z.min(), vmax=Z.max()), alpha=0.4)\n",
    "fig.colorbar(cs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We'll start with a random point $x$. We'll then use a Normal distribution to draw 20 points around $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x = np.random.rand(2) * 4 - 2\n",
    "print(\"position: \", x, \"fitness :\", himmelblau(x[0], x[1]))\n",
    "x_t = np.array([x + np.random.normal(size=(2,)) for i in range(20)])\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "cs = plt.contour(X, Y, Z, levels=100, cmap='Spectral', norm=colors.Normalize(vmin=Z.min(), vmax=Z.max()), alpha=0.4)\n",
    "plt.scatter(x_t[:, 0], x_t[:, 1], c='b')\n",
    "plt.scatter(x[0], x[1], c='r')\n",
    "fig.colorbar(cs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Having this population of points will enable us to explore the space around $x$ before moving on to the next step, which can better inform movement in the search space. However, these points might overlap or cover spaces that have already been explored. Generating too many points could slow down search, requiring more evaluations of the objective function. In many applications, that is costly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Performing random optimization but sampling more than 1 point leads to the $(1+\\lambda)$ Evolutionary Strategy. \n",
    "\n",
    "    Initialize x randomly in ‚Ñù\n",
    "    while not terminate\n",
    "        x_p = x\n",
    "        for i in [1,Œª]\n",
    "            x_i = x_p + ùëÅ(0, 1)\n",
    "            if f(x_i) < f(x)\n",
    "                x = x_i\n",
    "        x_p = x\n",
    "    return x_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the scope of evolutionary algorithms, we'll refer to $x$ as a parent and all $x'$ points as offspring. Each point can also be referred to as an individual in a population, and $f(x)$ is called the fitness of the individual. Each iteration of the algorithm is called a generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "The $(\\mu/\\rho,\\lambda)$ or $(\\mu/\\rho+\\lambda)$ notation signifies the configuration of parents and offspring. In this notation, $\\mu$ is the number of parents, $\\rho$ is the number of parents involved in creating the offspring, and $\\lambda$ is the number of offspring. $(\\mu/\\rho+\\lambda)$ means the parents can be included in the next population, whereas $(\\mu/\\rho,\\lambda)$ means the parents are not included. $(1+\\lambda)$ therefore means that 1 parent is involved in creating a population of $\\lambda$ offspring and can be included in the next generation (ie, if $f(x) < f(x') \\forall x'$, $x$ does not change).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def oneplus_lambda(x, f, gens=200, lam=20):\n",
    "    x_best = x\n",
    "    f_best = f(x)\n",
    "    fits = np.zeros(gens)\n",
    "    for g in range(gens):\n",
    "        N = np.random.normal(size=(lam, len(x)))\n",
    "        for i in range(lam):\n",
    "            ind = x + N[i, :]\n",
    "            fi = f(ind)\n",
    "            if fi < f_best:\n",
    "                f_best = fi\n",
    "                x_best = ind\n",
    "        x = x_best\n",
    "        fits[g] = f_best\n",
    "    return fits, x_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's run the $(1+\\lambda)$ ES on the himmelblau function and record the evolution over multiple trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "f = himmelblau_v\n",
    "runs = []\n",
    "for i in range(5):\n",
    "    x = np.random.rand(2)*10-5\n",
    "    fits, _ = oneplus_lambda(x, f)\n",
    "    runs += [fits]\n",
    "runs = np.array(runs)\n",
    "runs_mean = np.mean(runs, 0)\n",
    "runs_std = np.std(runs, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6))\n",
    "plt.fill_between(range(len(runs[0])), runs_mean+0.5*runs_std, runs_mean-0.5*runs_std, alpha=0.5)\n",
    "plt.plot(range(len(runs[0])), runs_mean)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Exercise</h3>\n",
    "    Study the impact of the $\\lambda$ parameter by modifying it and re-running the optimization. What is the best $\\lambda$ value for the Himmelblau problem?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Exercise</h3>\n",
    "    Visualize the two other functions, Rosenbrock and the warped Rastrigin function. Try the $(1+\\lambda)$ algorithm on these other functions.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <a name=\"gradient\">2.</a> Approximating the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In the $(1+\\lambda)$ ES, we move from the best point in a population to the best point in the next randomly sampled population. However, with the population information, we can do better than that; we can move in the direction of the gradient approximated by the fitness values of the population. This can also better use the full population information, combining all fitness information instead of selecting the best fitness value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "x = np.random.rand(2) * 4 - 2\n",
    "x_t = np.array([x + np.random.normal(size=(2,)) for i in range(10)])\n",
    "fits = himmelblau(x_t[:, 0], x_t[:, 1])\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "norm = colors.Normalize(vmin=Z.min(), vmax=Z.max())\n",
    "cs = plt.contour(X, Y, Z, levels=100, cmap='Spectral', alpha=0.4, norm=norm)\n",
    "plt.quiver(x_t[:, 0], x_t[:, 1], x_t[:, 0]-x[0], x_t[:, 1]-x[1],\n",
    "           fits, scale=fits/100, scale_units='xy', cmap='Spectral', norm=norm)\n",
    "plt.scatter(x[0], x[1], c='k')\n",
    "fig.colorbar(cs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "First we normalize all fitness values by the average $\\mu(f(x))$ and standard deviation $\\sigma(f(x))$ of the population. This is to make the movement based on the relative fitness in the population as opposed to the absolute fitness in the search space.\n",
    "\n",
    "$A = \\frac{f(x) - \\mu(f(x))}{\\sigma(f(x))}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We'll then define a vector at each point, $x_i - x$. Note that since $x_i = x + N(0, 1)$, we could store just this original random sampled point from $N(0, 1)$ as our vector.\n",
    "\n",
    "$N_i = x_i - x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We multiply $A$ by all individual vectors which scales the vector magnitude by the relative fitness. Finally, we sum all vectors and divide by $\\lambda$ to get the weighted average.\n",
    "\n",
    "$\\frac{1}{\\lambda}\\sum_i A_i N_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is equivalent to the dot product of the fitness vector $A$ and the positional vector $N$. We consider this an approximation of the gradient of $f$, as it tells us the change in $f$ over movement in $x$. We can descend along this gradient approximation to minimize:\n",
    "\n",
    "$\\nabla f \\approx -\\frac{A \\cdot N}{\\lambda}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "A = (fits - np.mean(fits)) / np.std(fits)\n",
    "N = x_t - x\n",
    "G = -np.dot(A, N) / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6))\n",
    "norm = colors.Normalize(vmin=Z.min(), vmax=Z.max())\n",
    "cs = plt.contour(X, Y, Z, levels=100, cmap='Spectral', alpha=0.4, norm=norm)\n",
    "plt.quiver(x_t[:, 0], x_t[:, 1], x_t[:, 0]-x[0], x_t[:, 1]-x[1],\n",
    "           fits, scale=fits/100, scale_units='xy', cmap='Spectral', norm=norm)\n",
    "plt.quiver(x[0], x[1], G[0], G[1])\n",
    "plt.scatter(x[0], x[1], c='k')\n",
    "fig.colorbar(cs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We have our approximate gradient direction, but how far should we move in it? We'll define a learning rate variable $\\alpha$ as the fixed magnitude of movement for now, so our update of $x$ is then\n",
    "\n",
    "$x = x - \\alpha\\frac{A \\cdot N}{\\lambda}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's put this all together as an Evolutionary Strategy. It is a $(\\mu, \\lambda)$ ES as multiple individuals $\\mu$ inform the next generation; however, these individuals are not kept for the next generation.\n",
    "\n",
    "    Initialize x randomly in ‚Ñù\n",
    "    while not terminate\n",
    "        for i in [1,Œª]\n",
    "            N_i = ùëÅ(0, 1)\n",
    "            F_i = f(x + N_i)\n",
    "        A = (F‚àíùúá(F))/ùúé(F)\n",
    "        x = x - ùõº(A‚ãÖN)/ùúÜ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def mu_lambda(x, fitness, gens=200, lam=10, alpha=0.2, verbose=False):\n",
    "    x_best = x\n",
    "    f_best = fitness(x)\n",
    "    fits = np.zeros(gens)\n",
    "    for g in range(gens):\n",
    "        N = np.random.normal(size=(lam, len(x)))\n",
    "        F = np.zeros(lam)\n",
    "        for i in range(lam):\n",
    "            ind = x + N[i, :]\n",
    "            F[i] = fitness(ind)\n",
    "            if F[i] < f_best:\n",
    "                f_best = F[i]\n",
    "                x_best = ind\n",
    "                if verbose:\n",
    "                    print(g, \" \", f_best)\n",
    "        fits[g] = f_best\n",
    "        mu_f = np.mean(F)\n",
    "        std_f = np.std(F)\n",
    "        A = F\n",
    "        if std_f != 0:\n",
    "            A = (F - mu_f) / std_f\n",
    "        x = x - alpha * np.dot(A, N) / lam\n",
    "    return fits, x_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "f = himmelblau_v\n",
    "runs = []\n",
    "for i in range(5):\n",
    "    x = np.random.randn(2)\n",
    "    fits, _ = mu_lambda(x, f)\n",
    "    runs += [fits]\n",
    "runs = np.array(runs)\n",
    "runs_mean = np.mean(runs, 0)\n",
    "runs_std = np.std(runs, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6))\n",
    "plt.fill_between(range(len(runs[0])), runs_mean+0.5*runs_std, runs_mean-0.5*runs_std, alpha=0.5)\n",
    "plt.plot(range(len(runs[0])), runs_mean)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Exercise</h3>\n",
    "    \n",
    "Study the impact of the $\\alpha$ parameter by modifying it and re-running the optimization. Compare the best parameters found with the results from the $(1+\\lambda)$ ES.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <a name=\"canonical\">3.</a> Canonical ES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can make two observations about our current $(\\mu, \\lambda)$ Evolutionary Strategy:\n",
    "\n",
    "1. Not all $\\lambda$ points may be useful for the centroid update\n",
    "2. Exact gradient information may be less useful than relative *rank* of individual fitness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's illustrate this point in the Himmelblau function. Instead of using all $\\lambda$ points, we'll use a smaller value of $\\mu$, for example $\\frac{1}{2}\\lambda$. We'll also create a list of weights which exponentially decreases from the first to last weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mu = 5\n",
    "w = np.log(mu + 1/2) - np.log(np.arange(1, mu+1))\n",
    "w /= np.sum(w)\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As before, we create a random starting point and then add Gaussian noise to create offspring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "x = np.random.rand(2) * 4 - 2\n",
    "N = np.random.normal(size=(10,2))\n",
    "x_t = x + N\n",
    "fits = himmelblau(x_t[:, 0], x_t[:, 1])\n",
    "fits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now, however, instead of directly using fitness values, we'll select the best individuals and update using their weighted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sorted_ids = np.argsort(fits)\n",
    "weighted = N[sorted_ids[:mu]] * w.reshape(mu, 1)\n",
    "weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6))\n",
    "norm = colors.Normalize(vmin=Z.min(), vmax=Z.max())\n",
    "cs = plt.contour(X, Y, Z, levels=100, cmap='Spectral', alpha=0.4, norm=norm)\n",
    "plt.quiver(x_t[:, 0], x_t[:, 1], x_t[:, 0]-x[0], x_t[:, 1]-x[1],\n",
    "           fits, scale=fits/100, scale_units='xy', cmap='Spectral', norm=norm, alpha=0.4)\n",
    "plt.quiver(x[0]+weighted[:, 0], x[1]+weighted[:, 1], weighted[:, 0], weighted[:, 1],\n",
    "           w, scale_units='xy', cmap='Spectral', norm=norm)\n",
    "plt.scatter(x[0], x[1], c='k')\n",
    "fig.colorbar(cs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Exercise</h3>\n",
    "    \n",
    "Consider the Rosenbrock function. How do you expect the gradient rankings to compare to the gradient information? Will they be very similar or different? Plot a sample to investigate.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The first point is clear: using all points in the centroid update could pull it away from nearby optima. The second point is a bit harder to understand, but motivation is this: any transformation to the search space which **preserves rank** for the top $\\mu$ individuals does not change the update. This invariance property is helpful to search because it increases the predictive power of the fitness by inducing problem equivalence classes.\n",
    "\n",
    "Hansen, N., Ros, R., Mauny, N., Schoenauer, M., & Auger, A. (2011). Impacts of invariance in search: When CMA-ES and PSO face ill-conditioned and non-separable problems. Applied Soft Computing, 11(8), 5755-5769."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Incorporating these two changes to the previous $(\\mu, \\lambda)$ ES brings us an ES referred to as Canonical ES\n",
    "\n",
    "<img src=\"imgs/canonical.png\" width=\"40%\" height=\"auto\">\n",
    "\n",
    "Chrabaszcz, P., Loshchilov, I., & Hutter, F. (2018, July). Back to basics: benchmarking canonical evolution strategies for playing Atari. In Proceedings of the 27th International Joint Conference on Artificial Intelligence (pp. 1419-1426). [code](https://github.com/PatrykChrabaszcz/Canonical_ES_Atari)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "While this evolutionary strategy is simple, it has been used to evolve deep neural networks capable of playing Atari games. It is competitive with an ES developed by OpenAI, and they are both competitive with deep reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"imgs/canonical_results.png\" width=\"80%\" height=\"auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<h3>Embarrassingly parallel ES</h3>\n",
    "\n",
    "All ES examples we've seen so far use sequential evaluation of individuals in a single thread. However, evaluations can be done entirely in parallel. Evaluation is by far the most computationally expensive part of the algorith and the fact that it is embarrassingly parallel means that the wall clock time of evolutionary algorithms in general can be greatly reduced by parallelizing over multiple threads.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can use this weighted sum gradient estimate to make a new ES function for Canonical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def canonical(x, fitness, gens=200, mu=10, lam=20, verbose=False):\n",
    "    # best values and fitness vector for logging\n",
    "    x_best = x\n",
    "    f_best = fitness(x)\n",
    "    fits = np.zeros(gens)\n",
    "    \n",
    "    # calculate weight list\n",
    "    w = np.log(mu + 1/2) - np.log(np.arange(1, mu+1))\n",
    "    w /= np.sum(w)\n",
    "    \n",
    "    # generation loop\n",
    "    for g in range(gens):\n",
    "        # calculate new individuals\n",
    "        N = np.random.normal(size=(lam,2))\n",
    "        x_t = x + N\n",
    "        # evaluate individuals\n",
    "        F = [fitness(x_t[i, :]) for i in range(lam)]\n",
    "        # store best individual for logging\n",
    "        for i in range(lam):\n",
    "            if F[i] < f_best:\n",
    "                f_best = F[i]\n",
    "                x_best = x_t[i]\n",
    "                if verbose:\n",
    "                    print(g, \" \", f_best)\n",
    "        fits[g] = f_best\n",
    "        # use weighted sum to update center\n",
    "        sorted_ids = np.argsort(F)\n",
    "        weighted = N[sorted_ids[:mu]] * w.reshape(mu, 1)\n",
    "        s = np.mean(weighted, axis=0)\n",
    "        x = x + s\n",
    "        \n",
    "    return fits, x_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Exercise</h3>\n",
    "    \n",
    "Compare the $(1+\\lambda)$, $(\\mu,\\lambda)$, and Canonical ES on the Rosenbrock and warped Rastrigin functions. Is there a clearly superior ES out of these three?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def compare_es(es_list, f, n_runs=5):\n",
    "    means = []\n",
    "    devs = []\n",
    "    max_g = 0\n",
    "    for e in range(len(es_list)):\n",
    "        runs = []\n",
    "        for i in range(5):\n",
    "            x = np.random.rand(2)*10-5\n",
    "            fits, _ = es_list[e](x, f)\n",
    "            runs += [fits]\n",
    "            max_g = np.maximum(max_g, len(fits))\n",
    "        runs = np.array(runs)\n",
    "        runs_mean = np.mean(runs, 0)\n",
    "        runs_std = np.std(runs, 0)\n",
    "        means.append(runs_mean)\n",
    "        devs.append(runs_std)\n",
    "\n",
    "    means = np.array(means)\n",
    "    devs = np.array(devs)\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    for e in range(len(es_list)):\n",
    "        plt.fill_between(range(max_g), means[e]+0.5*devs[e], means[e]-0.5*devs[e], alpha=0.5)\n",
    "        plt.plot(range(max_g), means[e], label=str(e))\n",
    "    plt.legend()\n",
    "    plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "compare_es([oneplus_lambda, mu_lambda, canonical], himmelblau_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <a name=\"cmaes\">4.</a> CMAES part 1: Maximum Likelihood Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Covariance Matrix Adaptation Evolutionary Strategy, or CMA-ES [1, 2], is one of the most well-known evolutionary algorithms in general and is a state-of-the-art algorithm for continuous optimization. The strength of this method is that it adapts the distribution it uses to generate the next population based on the current distribution of individuals. Until now, we were limited to a Normal distribution with a static $\\sigma$. The adaptive distribution of CMA-ES means it will cross search spaces faster and narrow in more exactly on optimal points.\n",
    "\n",
    "[1] Hansen, Nikolaus, and Andreas Ostermeier. \"Adapting arbitrary normal mutation distributions in evolution strategies: The covariance matrix adaptation.\" Proceedings of IEEE international conference on evolutionary computation. IEEE, 1996.\n",
    "\n",
    "[2] Hansen, Nikolaus, and Andreas Ostermeier. \"Completely derandomized self-adaptation in evolution strategies.\" Evolutionary computation 9.2 (2001): 159-195."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Specifically, the things that CMA-ES improves over the previous Evolutionary Strategies we've seen is that it:\n",
    "+ combines information from multiple individuals (as in $(\\mu, \\lambda)$ and Canonical ES)\n",
    "+ combines information from multiple generations\n",
    "+ transforms the distribution of the new population to match the search space\n",
    "+ adapts the step size to prevent premature convergence\n",
    "\n",
    "One improvement in CMA-ES which we've already seen is the use of **fitness rank** instead of direct fitness for the update. However, instead of directly updating a centroid, CMA-ES updates a covariance matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "CMA-ES uses two principles to achieve this: maximum likelihood estimation and step-size control. We'll start with maximum likelihood estimation, which consists of increasing the probability of successful points. CMA-ES will update the population center by taking the weighted average of high-fitness individuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = np.arange(-5, 5, 0.1)\n",
    "Y = np.arange(-5, 5, 0.1)\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "Z = himmelblau(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lam = 20\n",
    "x = np.random.rand(2) * 4 - 2\n",
    "N = np.random.normal(size=(lam,2))\n",
    "x_t = x + N\n",
    "fits = himmelblau(x_t[:, 0], x_t[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "First, we select the top $\\mu$ individuals based on their fitness values. We'll then weight their importance by their fitness rank, using a static weight vector $w$. This weight vector $w$ is a hyperparameter in CMA-ES, but there is a standard normalized logarithmic weight scale used by most. Note that the weight is not their fitness values but rather just the rank. This has been demonstrated to aid in search in ill-formed search spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mu = 10\n",
    "sorted_ids = np.argsort(fits)\n",
    "w = np.log(mu + 1/2) - np.log(np.arange(1, mu+1))\n",
    "w /= np.sum(w)\n",
    "weighted = N[sorted_ids[:mu]] * w.reshape(mu, 1)\n",
    "s = np.mean(weighted, axis=0)\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This $s$ direction vector is similar to the gradient estimation from the last exercise. However, in CMA-ES, only the top $\\mu$ individuals are used to calculate $s$, and instead of directly using fitness values, $s$ is calculated using weight based on fitness rank. We can compare the two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "A = (fits - np.mean(fits)) / np.std(fits)\n",
    "G = -np.dot(A, N) / lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6))\n",
    "norm = colors.Normalize(vmin=Z.min(), vmax=Z.max())\n",
    "cs = plt.contour(X, Y, Z, levels=100, cmap='Spectral', alpha=0.4, norm=norm)\n",
    "plt.scatter(x_t[:, 0], x_t[:, 1], c=fits, cmap='Spectral', norm=norm)\n",
    "plt.quiver(x[0], x[1], G[0], G[1], color='b', scale=4)\n",
    "plt.quiver(x[0], x[1], s[0], s[1], color='g', scale=1)\n",
    "plt.scatter(x[0], x[1], c='k')\n",
    "fig.colorbar(cs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "While we're only using the fitness information for a ranked weighting, this direction vector $s$ can be seen as a gradient approximation. As before, we can use this gradient information to update the center of our distribution. This is the first step of maximum likelihood estimation, aligning the distribution center with the weighted average of the best solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x = x + s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A fundamental idea behind CMA-ES is to estimate the covariance between the selected samples, and to use this covariance matrix to shape the next distribution. A covariance matrix measures the joint variability between the different dimensions of the objective function. Covariance matrices are outside of the scope of this class, but this article has more details:\n",
    "https://datascienceplus.com/understanding-the-covariance-matrix/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We start CMA-ES with the identity matrix as the initial covariance matrix, assuming each variable is independent and has a standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "C = np.eye(2)\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "At each step, we update $C = C + ss^T$. This will use the weighted sample average as an estimate of the covariance. In CMA-ES, it is referred to as the rank one covariance update. For problems of higher dimension, there is an additional matrix update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "C = C + np.outer(s, (s).T)\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The eigen decomposition of the covariance matrix is then used to transform the Normal distribution to match the search space. This is the second part of maximum likelihood estimation: by pulling samples from a distribution fit to performant parts of the search space, there is a higher chance of sampling good individuals. The mean of the distribution $x$ and the shape given by $C$ together maximize this likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "N = np.random.normal(size=(lam, 2))\n",
    "x_N = x + N\n",
    "eigenvalues, eigenvectors = np.linalg.eig(C)\n",
    "x_C = x + np.array([np.dot(eigenvectors, np.sqrt(eigenvalues) * N[i, :]) for i in range(lam)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6))\n",
    "norm = colors.Normalize(vmin=Z.min(), vmax=Z.max())\n",
    "cs = plt.contour(X, Y, Z, levels=100, cmap='Spectral', alpha=0.4, norm=norm)\n",
    "plt.scatter(x_N[:, 0], x_N[:, 1], c='k', alpha=0.3)\n",
    "plt.scatter(x_C[:, 0], x_C[:, 1], c='b')\n",
    "plt.scatter(x[0], x[1], c='r')\n",
    "fig.colorbar(cs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can consider this improvement alone and add it to the Canonical ES defined above. The only modification is that, instead of sampling from the Normal distribution, we'll transform the normal distribution using the covariance matrix first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def covariance_es(x, fitness, gens=200, mu=10, lam=20, verbose=False):\n",
    "    # best values and fitness vector for logging\n",
    "    x_best = x\n",
    "    f_best = fitness(x)\n",
    "    fits = np.zeros(gens)\n",
    "    \n",
    "    # calculate weight list and first covariance matrix\n",
    "    w = np.log(mu + 1/2) - np.log(np.arange(1, mu+1))\n",
    "    w /= np.sum(w)\n",
    "    C = np.eye(2)\n",
    "    \n",
    "    # generation loop\n",
    "    for g in range(gens):\n",
    "        # calculate new individuals\n",
    "        N = np.random.normal(size=(lam,2))\n",
    "        x_N = x + N\n",
    "        eigenvalues, eigenvectors = np.linalg.eig(C)\n",
    "        x_C = x + np.array([np.dot(eigenvectors, np.sqrt(eigenvalues) * N[i, :]) for i in range(lam)])\n",
    "        # evaluate individuals\n",
    "        F = [fitness(x_C[i, :]) for i in range(lam)]\n",
    "        # store best individual for logging\n",
    "        for i in range(lam):\n",
    "            if F[i] < f_best:\n",
    "                f_best = F[i]\n",
    "                x_best = x_C[i]\n",
    "                if verbose:\n",
    "                    print(g, \" \", f_best)\n",
    "        fits[g] = f_best\n",
    "        # use weighted sum to update center\n",
    "        sorted_ids = np.argsort(F)\n",
    "        weighted = N[sorted_ids[:mu]] * w.reshape(mu, 1)\n",
    "        s = np.mean(weighted, axis=0)\n",
    "        x = x + s\n",
    "        C = C + np.outer(s, (s).T)\n",
    "        \n",
    "    return fits, x_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Exercise</h3>\n",
    "    \n",
    "Compare this new ES to Canonical ES. Does the inclusion of the Covariance matrix improve the results?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <a name=\"cmaes\">5.</a> CMAES part 2: Step size adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To understand the second main feature of CMA-ES, step size adaptation, let's go back to the first ES defined, the $(1+\\lambda)$. We can include an important hyperparameter in this method: the Normal distribution size or standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def oneplus_lambda(x, f, std=1, gens=200, lam=20):\n",
    "    x_best = x\n",
    "    f_best = f(x)\n",
    "    fits = np.zeros(gens)\n",
    "    for g in range(gens):\n",
    "        N = np.random.normal(size=(lam, len(x))) * std\n",
    "        for i in range(lam):\n",
    "            ind = x + N[i, :]\n",
    "            fi = f(ind)\n",
    "            if fi < f_best:\n",
    "                f_best = fi\n",
    "                x_best = ind\n",
    "        x = x_best\n",
    "        fits[g] = f_best\n",
    "    return fits, x_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By comparing different values of standard deviation, we can see that this has a major effect on the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "f0 = lambda x, f: oneplus_lambda(x, f, std=0.01)\n",
    "f1 = lambda x, f: oneplus_lambda(x, f, std=0.1)\n",
    "f2 = lambda x, f: oneplus_lambda(x, f, std=1)\n",
    "f3 = lambda x, f: oneplus_lambda(x, f, std=10)\n",
    "\n",
    "compare_es([f0, f1, f2, f3], himmelblau_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The second main feature in CMA-ES besides maximum likelihood estimation is step-size control. This is very similar to the temperature parameter in simulated annealing: it updates over time to allow for either more exploration or more precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Exercise</h3>\n",
    "    \n",
    "Make the step size change during the $(1+\\lambda)$ ES for the Himmelblau function. You can try decreasing `std` by a small amount each step, or using exponential decay, or a rule like the one in simulated annealing. Then, once it works, apply this strategy to Rosenbrock. Does that beat the standard $(1+\\lambda)$?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The step size control used in CMA-ES uses the gradient estimate to determine if the step size should be expanded (exploration) or decreased (exploitation). [This article](http://www.cmap.polytechnique.fr/~nikolaus.hansen/es-overview-2015.pdf) gives an overview of different ES, including the step size control used in CMA-ES. The following algorithm is a standard $(\\mu, \\lambda)$ algorithm with the step size control added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"imgs/step_size.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We have the two parts of CMA-ES now: maximum likelihood estimation and step-size control. These two combine to modify the distribution, both in shape and size, over the search. As we see in the animation below, CMA-ES adapts the distribution to the search space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://user-images.githubusercontent.com/5564044/73486622-db5cff00-43e8-11ea-98fb-8246dbacab6d.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"imgs/cmaes.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To see CMA-ES in practice, we'll use the pycma package (https://github.com/CMA-ES/pycma). If you haven't installed it, do so here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install cma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`pycma` is actively maintained by the CMA-ES author and has many more algorithm tricks and features than we've discussed here. [4] provides a good review of different CMA-ES modifications.\n",
    "\n",
    "[4] Hansen, Nikolaus. \"The CMA evolution strategy: a comparing review.\" Towards a new evolutionary computation. Springer, Berlin, Heidelberg, 2006. 75-102."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import cma\n",
    "es = cma.CMAEvolutionStrategy(2 * [0], 0.1, {'popsize': 20, 'verb_disp': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "solutions = np.array(es.ask())\n",
    "es.tell(solutions, [himmelblau(x[0], x[1]) for x in solutions])\n",
    "es.disp_annotation()\n",
    "es.disp();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = plt.subplot()\n",
    "norm = colors.Normalize(vmin=Z.min(), vmax=Z.max())\n",
    "cs = plt.contour(X, Y, Z, levels=100, cmap='Spectral', alpha=0.4, norm=norm)\n",
    "plt.scatter(solutions[:, 0], solutions[:, 1])\n",
    "fig.colorbar(cs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def generation(i, es, ax):\n",
    "    solutions = np.array(es.ask())\n",
    "    es.tell(solutions, [himmelblau(x[0], x[1]) for x in solutions])\n",
    "    \n",
    "    ax.clear()\n",
    "    ax.contour(X, Y, Z, levels=100, cmap='Spectral', alpha=0.4, norm=norm)\n",
    "    ax.scatter(solutions[:, 0], solutions[:, 1])\n",
    "    ax.set_ylim(-5, 5)\n",
    "    ax.set_xlim(-5, 5)\n",
    "    ax.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "es = cma.CMAEvolutionStrategy(2 * [0], 0.1, {'popsize': 20})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "from IPython import display\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = plt.subplot()\n",
    "fig.tight_layout()\n",
    "frames = 20\n",
    "animator = animation.FuncAnimation(fig, generation, fargs=(es, ax), frames=frames, interval=250, blit=False)\n",
    "display.display(display.HTML(animator.to_html5_video()))\n",
    "plt.close();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "ES, and particularly CMA-ES, have been widely used in many applications. Any problem which can be modelled as a continuous optimizaton problem can use an evolutionary strategy, and CMA-ES is the state of the art for ES. Here is a short list of applications:\n",
    "    \n",
    "+ Gagn√©, C., Beaulieu, J., Parizeau, M., & Thibault, S. (2008). Human-competitive lens system design with evolution strategies. Applied Soft Computing, 8(4), 1439-1452.\n",
    "+ Bayer, P., & Finkel, M. (2007). Optimization of concentration control by evolution strategies: Formulation, application, and assessment of remedial solutions. Water resources research, 43(2).\n",
    "+ K√§mpf, J. H., & Robinson, D. (2009). A hybrid CMA-ES and HDE optimisation algorithm with application to solar energy potential. Applied Soft Computing, 9(2), 738-745.\n",
    "+ Maki, A., Sakamoto, N., Akimoto, Y., Nishikawa, H., & Umeda, N. (2020). Application of optimal control theory based on the evolution strategy (CMA-ES) to automatic berthing. Journal of Marine Science and Technology, 25(1), 221-233.\n",
    "+ Loshchilov, I., & Hutter, F. (2016). CMA-ES for hyperparameter optimization of deep neural networks. arXiv preprint arXiv:1604.07269.\n",
    "+ Fukagata, K., Kern, S., Chatelain, P., Koumoutsakos, P., & Kasagi, N. (2008). Evolutionary optimization of an anisotropic compliant surface for turbulent friction drag reduction. Journal of Turbulence, (9), N35."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Exercise</h3>\n",
    "    \n",
    "Compare CMA-ES to the $(1+\\lambda)$, $(\\mu, \\lambda)$, and Canonical ES. Plot the mean and standard deviation over 5 independent runs, as before.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
